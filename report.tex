\title{\textbf{Final Report}\\Higher-Order Genetic Programming \\for Semantic Unifiers}

\author{Larry Diehl}
\date{\today}

\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[hscale=0.7,vscale=0.8]{geometry}

\newcommand{\n}[1]{\textrm{#1}}

\begin{document}

\maketitle

\section{Project Description}

My project is about solving for existential variables in one or more
equations, such that the equations become satisfied after semantic
reductions have occurred. This ends up being a restricted form of a
semantic unification problem that has several universally quantified
variables and a single existential variable. The left hand-side of the
equation is always the existential variable applied to some number of
arbitrary expressions, and the right-hand side is an arbitrary
expression. A futher restriction is that the aforementioned arbitrary
expressions cannot include the existential variable.

Notably, the a solution for an existential variable can be a
higher-order term -- a function that takes other functions as
arguments. I use genetic programming, mostly replicating the work by
Matthias Fuchs in his ``Evolving Combinators'' paper, to solve for the
existential variables. The interesting part of this kind of GP is that
higher-order solutions can be evolved. In standard GP with lambda
terms, a crossover could result in variables unintentionally capturing
other variables, or referencing variables not currently in scope. In
Fuchs' GP, the population of candidate solutions are higher-order
combinators. Trees of applications of combinators implicitly
manipulate variables (via point-free programming), but crossover is
not problematic because variables are never explicitly bound or
referenced.

\subsection{Combinatory Logic Problems}

Most of my time was spent using the project to solve combinatory logic
problems. For example, the following combinatory logic problem asks for a term
consisting exclusively of combinators that is equivalent to the
desired lambda-expression.

$$
T = \lambda a,b . ~ b ~ a
$$

A solution to this problem is the combinator below.

$$
(S(K(S((SK)K))))K
$$

In my system, this problem is represented by the following equation.

$$
\forall a,b . ~ \exists T . ~ T ~ a ~ b = b ~ a
$$

Besides functions, I was also interested in evolving problems
containing datatypes. Notably, problems with datatypes can be
considered a subset of combinatory logic problems by using church
encodings. For example, below if the church-encoding of true and
false.

\begin{align*}
\n{true} &= \lambda a. \lambda b. a\\
\n{false} &= \lambda a. \lambda b. b
\end{align*}

Using Church-encodings, you can represent the ``and'' problem as
follows.

$$
\forall a,b . ~ \exists \n{and} . ~ \n{and} ~ a ~ b = a ~ b ~ a
$$

My algorithm has to give solutions in terms of some primitive set of
combinators. I started with just S and K, which are sound and complete
for the implicational fragment of intuitionistic logic. However, after
experimentation I ended up discovering that S, K, I, B, and C are a
good base set. The combinators B and C can be seen as common/useful
specializations of S, and are due to the logician Moses Sch{\"o}nfinkel.

\begin{align*}
S &= \lambda a,b,c . ~ a ~ c ~ (b ~ c)\\
K &= \lambda a,b . ~ a\\
I &= \lambda a . ~ a\\
B &= \lambda a,b,c . ~ a ~ (b ~ c)\\
C &= \lambda a,b,c . ~ a ~ c ~ b
\end{align*}

The GP system is based on term-rewriting, so the axioms above are
actually encoded as follows.

\begin{align*}
S ~ a ~ b ~ c &= a ~ c ~ (b ~ c)\\
K ~ a ~ b &= a\\
I ~ a &= a\\
B ~ a ~ b ~ c &= a ~ (b ~ c)\\
C ~ a ~ b ~ c &= a ~ c ~ b
\end{align*}


\subsection{Hilbert System Problems}

Besides Church-encoded problems, I also wanted to solve problems with
native datatypes (to avoid the inneficiencies associated with
encodings, and to be more similar to real languages).
To do so, I ended up using a Hilbert system. A Hilbert system replaces
the standard implication introduction and elimination rules with the
combinators S and K, and axiomatically extends the system with other
native types. For example, below is the Hilbert system for a language
with booleans.

\begin{align*}
S ~ a ~ b ~ c &= a ~ c ~ (b ~ c)\\
K ~ a ~ b &= a\\
\n{If} ~ \n{true} ~ a ~ b &= a\\
\n{If} ~ \n{false} ~ a ~ b &= b
\end{align*}

Now, my system can encode the ``and'' problem using native datatypes of a
Hilbert system as follows.

\begin{align*}
\forall a,b . ~ \exists \n{and} .\\
\n{and} ~ \n{true} ~ \n{true} &= \n{true}\\
&\land\\
\n{and} ~ \n{false} ~ b &= \n{false}\\
&\land\\
\n{and} ~ a ~ \n{false} &= \n{false}
\end{align*}

The problem above demonstrates two extensions of my system, where we
can give multiple fitness cases (multiple equations), and we can apply
arbitrary expressions (e.g. ``true'' instead of just variables) to the
desired existential on the left-hand side.

\subsection{Genetic Programming Algorithm}

The GP algorithm is a mostly-standard GP implementation. The novelty
(thanks to Kuchs) lies in the \textit{representation} of candidate
solutions, and the \textit{fitness function}. Recall that the RHS of
equations in problems can mention the universally quantified
variables, but the LHS cannot. This restrictions allows for the
candidate solutions to simply be abstract syntax trees with binary
branches (representing function application) and atoms at the leaves
(representing combinators). Notably, a leaf cannot mention a variable.

To compute fitness, we reduce the RHS and compare it to the result of
reducing the LHS after applying its arguments to the candidate
existential variable solution. However, the comparison is not merely a
binary comparison. Instead, it is a structural difference calculation
of the two resulting abstract syntax trees. Furthermore, there is a
weighting mechanism to assign higher fitness scores to solutions that
do not have some minimum number of combinators at the leaves (this
algorithm input is called the ``minimum structure'').

\subsubsection{My Changes}

Changes that I introduced into the algorithm include a 1-level
tournament selection (pick a parent by comparing the fitness of two
\textit{random} candidates)
This is known to perform better than Kuchs' selection (pick a parent
by selecting a candidate from the population with a bias towards the
fitter-end of the spectrum).
Additionally, Kuchs did not have mutation so I added it as an option
for the user.
I also added a novel option to partially evaluate population
candidates during the algorithm run, thus removing bloat. Finally,
Kuchs performed crossover by first crossing over parents, then
recursively trying again if the maximum depth requirement was
violated. I improved the algorithm by only choosing from crossover
points that result in a child satisfying the maximum depth
requirement. This is accomplished by filtering the depth of all
subnodes of the second parent by whether they are
less-than-or-equal-to the maximum depth minus the distance from the
point of the first parent to the root.

\subsection{Portion of Project Completed}

I implemented all the functionality I set out to for the project, but
I only tested boolean problems in the Hilbert system portion (I also wanted to
test more boolean problems and numeric problems). Additionally, I
evaluated my results by manual inspection of output data, and I wanted
to make some fancy graphs to more easily identify good learning.

\section{Unanticipated Obstacles}

About midway through the course I added partial evaluation to my GP
algorithm. Later I noticed that the algorithm was slow on large
problems, and turning on mutation would sometimes cause infinite
loops. The solution to this mystery was that partial normalization can
sometimes \textit{increase} the solution size, making it bigger than
the maximum depth requirement. Once that invariant was violated, other
parts of the algorithm that depended on it could loop. I got rid of
this problem by only using the partial evaluation result if its depth
was smaller than the original candidate.

\section{Lessons Learned}

\begin{itemize}
\item Always test with a random or brute force algorithm first. This can
help you find which problems are difficult in the first place, so you
can focus on making your algorithm to better on those.
\item Use dynamic checks of invariants in your algorithm in case future
changes break them.
\end{itemize}

\section{Code Statistics}

\section*{Appendix}
\appendix

\section{Evo.hs}

\end{document}

